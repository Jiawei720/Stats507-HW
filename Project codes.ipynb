{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available splits: dict_keys(['train'])\n",
      "Training samples: 21069\n",
      "Validation samples: 2341\n",
      "Original train size: 21069\n",
      "Original validation size: 2341\n",
      "Reduced train size: 2106\n",
      "Reduced validation size: 234\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "\n",
    "raw_datasets = load_dataset(\"microsoft/cats_vs_dogs\")\n",
    "print(\"Available splits:\", raw_datasets.keys())\n",
    "\n",
    "# If a validation split is not provided, create one from the training data.\n",
    "if \"validation\" not in raw_datasets:\n",
    "    train_val = raw_datasets[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "    train_datasets = train_val[\"train\"]\n",
    "    eval_datasets = train_val[\"test\"]\n",
    "else:\n",
    "    train_datasets = raw_datasets[\"train\"]\n",
    "    eval_datasets = raw_datasets[\"validation\"]\n",
    "\n",
    "# Check for a test split; if not available, set test_dataset to None.\n",
    "if \"test\" in raw_datasets:\n",
    "    test_datasets = raw_datasets[\"test\"]\n",
    "else:\n",
    "    test_datasets = None\n",
    "\n",
    "print(\"Training samples:\", len(train_datasets))\n",
    "print(\"Validation samples:\", len(eval_datasets))\n",
    "if test_datasets is not None:\n",
    "    print(\"Test samples:\", len(test_datasets))\n",
    "\n",
    "train_num = train_datasets.num_rows\n",
    "valid_num = eval_datasets.num_rows\n",
    "\n",
    "\n",
    "fraction = 1/10\n",
    "\n",
    "train_sample_size = int(train_num * fraction)\n",
    "valid_sample_size = int(valid_num * fraction)\n",
    "\n",
    "train_subset = train_datasets.shuffle(seed=42).select(range(train_sample_size))\n",
    "valid_subset = eval_datasets.shuffle(seed=42).select(range(valid_sample_size))\n",
    "\n",
    "\n",
    "ds = DatasetDict({\n",
    "    \"train\": train_subset,\n",
    "    \"validation\": valid_subset\n",
    "})\n",
    "\n",
    "print(\"Original train size:\", train_num)\n",
    "print(\"Original validation size:\", valid_num)\n",
    "print(\"Reduced train size:\", ds[\"train\"].num_rows)\n",
    "print(\"Reduced validation size:\", ds[\"validation\"].num_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'labels'],\n",
      "        num_rows: 2106\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['image', 'labels'],\n",
      "        num_rows: 234\n",
      "    })\n",
      "})\n",
      "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=306x220 at 0x1A8FD4D2B10>, 'labels': 1}\n"
     ]
    }
   ],
   "source": [
    "print(ds)\n",
    "\n",
    "print(ds['train'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x375>,\n",
       " 'labels': 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['validation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, RandomResizedCrop, RandomHorizontalFlip, ColorJitter, ToTensor, Normalize\n",
    "\n",
    "\n",
    "transform = Compose([\n",
    "    RandomResizedCrop(224),\n",
    "    RandomHorizontalFlip(),\n",
    "    ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "def preprocess(example):\n",
    "    # Check and handle unexpected data types\n",
    "    if isinstance(example['image'], list):\n",
    "        example['pixel_values'] = [transform(img) for img in example['image']]\n",
    "    else:\n",
    "        example['pixel_values'] = transform(example['image'])\n",
    "    \n",
    "    return example\n",
    "\n",
    "ds.reset_format()  # Ensure dataset is in the original format\n",
    "ds = ds.with_transform(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example['pixel_values'] for example in examples])\n",
    "    labels = torch.tensor([example['labels'] for example in examples])\n",
    "    return {'pixel_values': pixel_values, 'labels': labels}\n",
    "\n",
    "train_loader = DataLoader(ds['train'], batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(ds['validation'], batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, ViTForImageClassification\n",
    "\n",
    "\n",
    "model_name = \"google/vit-base-patch16-224-in21k\"\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,  \n",
    ")\n",
    "\n",
    "model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joe Wang\\AppData\\Local\\Temp\\ipykernel_15952\\2092795020.py:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joe Wang\\AppData\\Local\\Temp\\ipykernel_15952\\2092795020.py:37: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "c:\\Users\\Joe Wang\\anaconda3\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/66, Loss: 0.6902\n",
      "Batch 10/66, Loss: 0.2080\n",
      "Batch 20/66, Loss: 0.2022\n",
      "Batch 30/66, Loss: 0.1745\n",
      "Batch 40/66, Loss: 0.1785\n",
      "Batch 50/66, Loss: 0.1514\n",
      "Batch 60/66, Loss: 0.2556\n",
      "Epoch 1 Completed. Average Training Loss: 0.2064\n",
      "Validation Loss: 0.0700, Accuracy: 0.9701\n",
      "Epoch 2/5\n",
      "Batch 0/66, Loss: 0.0923\n",
      "Batch 10/66, Loss: 0.1762\n",
      "Batch 20/66, Loss: 0.2810\n",
      "Batch 30/66, Loss: 0.0957\n",
      "Batch 40/66, Loss: 0.2374\n",
      "Batch 50/66, Loss: 0.0403\n",
      "Batch 60/66, Loss: 0.0876\n",
      "Epoch 2 Completed. Average Training Loss: 0.1287\n",
      "Validation Loss: 0.1142, Accuracy: 0.9573\n",
      "Epoch 3/5\n",
      "Batch 0/66, Loss: 0.0756\n",
      "Batch 10/66, Loss: 0.1222\n",
      "Batch 20/66, Loss: 0.1085\n",
      "Batch 30/66, Loss: 0.0511\n",
      "Batch 40/66, Loss: 0.2304\n",
      "Batch 50/66, Loss: 0.2138\n",
      "Batch 60/66, Loss: 0.0805\n",
      "Epoch 3 Completed. Average Training Loss: 0.1163\n",
      "Validation Loss: 0.1244, Accuracy: 0.9444\n",
      "Epoch 4/5\n",
      "Batch 0/66, Loss: 0.1408\n",
      "Batch 10/66, Loss: 0.1315\n",
      "Batch 20/66, Loss: 0.2643\n",
      "Batch 30/66, Loss: 0.1977\n",
      "Batch 40/66, Loss: 0.1331\n",
      "Batch 50/66, Loss: 0.0441\n",
      "Batch 60/66, Loss: 0.0736\n",
      "Epoch 4 Completed. Average Training Loss: 0.1407\n",
      "Validation Loss: 0.2159, Accuracy: 0.9402\n",
      "Epoch 5/5\n",
      "Batch 0/66, Loss: 0.0324\n",
      "Batch 10/66, Loss: 0.0980\n",
      "Batch 20/66, Loss: 0.0408\n",
      "Batch 30/66, Loss: 0.0121\n",
      "Batch 40/66, Loss: 0.0738\n",
      "Batch 50/66, Loss: 0.1259\n",
      "Batch 60/66, Loss: 0.0649\n",
      "Epoch 5 Completed. Average Training Loss: 0.1024\n",
      "Validation Loss: 0.1249, Accuracy: 0.9487\n",
      "\n",
      "Training Summary:\n",
      "Epoch     Training Loss  Validation Loss     Accuracy  \n",
      "1         0.2064         0.0700              0.9701    \n",
      "2         0.1287         0.1142              0.9573    \n",
      "3         0.1163         0.1244              0.9444    \n",
      "4         0.1407         0.2159              0.9402    \n",
      "5         0.1024         0.1249              0.9487    \n"
     ]
    }
   ],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoModelForImageClassification, AutoImageProcessor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "scaler = GradScaler()\n",
    "\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\").to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=0.0002)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "epochs = 5\n",
    "train_batch_size = 128\n",
    "eval_batch_size = 128\n",
    "log_every = 10  \n",
    "\n",
    "\n",
    "results = {\"epoch\": [], \"step\": [], \"train_loss\": [], \"val_loss\": [], \"accuracy\": []}\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(f\"Epoch {epoch}/{epochs}\")\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_steps = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        inputs = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        total_steps += 1\n",
    "\n",
    "        \n",
    "        if batch_idx % log_every == 0:\n",
    "            print(f\"Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_train_loss = total_train_loss / total_steps\n",
    "    print(f\"Epoch {epoch} Completed. Average Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs.logits, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    \n",
    "    results[\"epoch\"].append(epoch)\n",
    "    results[\"step\"].append(total_steps)\n",
    "    results[\"train_loss\"].append(avg_train_loss)\n",
    "    results[\"val_loss\"].append(avg_val_loss)\n",
    "    results[\"accuracy\"].append(accuracy)\n",
    "\n",
    "\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(f\"{'Epoch':<10}{'Training Loss':<15}{'Validation Loss':<20}{'Accuracy':<10}\")\n",
    "for i in range(epochs):\n",
    "    print(f\"{results['epoch'][i]:<10}{results['train_loss'][i]:<15.4f}{results['val_loss'][i]:<20.4f}{results['accuracy'][i]:<10.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['pixel_values', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "print(batch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 94.87%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        \n",
    "        images = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        logits = outputs.logits  \n",
    "        \n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        \n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
